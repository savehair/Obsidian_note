根据您提供的教材、PPT课件及复习重点，以下是关于**生成式/判别式模型**、**有监督/无监督学习**、**集成学习（Bagging/Boosting）**以及**正则化**的详细对比与解析：

# 1 生成式模型 vs. 判别式模型 (Generative vs. Discriminative)

这两类模型的根本区别在于**建模对象**和**预测方式**不同。

- **判别式模型 (Discriminative Models)**
    
    - **核心思想**：直接学习输入 $X$ 到输出 $Y$ 的映射关系，即直接对==条件概率分布 **$P(Y|X)$** 或决策函数 $f(x)$ 进行建模==,。
    - **特点**：
        - 关注不同类别之间的“边界”差异。
        - 不能还原数据本身的分布，无法生成新数据。
        - 通常在分类任务中**准确率更高**，只需要有限的样本即可训练,。
    - **典型算法**：逻辑回归 (Logistic Regression)、支持向量机 (SVM)、BP神经网络、决策树、CRF,,。
- **生成式模型 (Generative Models)**
    
    - **核心思想**：先学习数据和标签的**联合概率分布 $P(X, Y)$**，然后通过贝叶斯公式推导出后验概率 $P(Y|X)$ 进行分类,。
    - **特点**：
        - 关注数据是如何产生的，刻画了每一类数据的分布特征。
        - 可以用于**生成新样本**（如GAN）。
        - 收敛速度快，当存在隐变量（Hidden Variables）时依然可以使用（如HMM）。
    - **典型算法**：朴素贝叶斯 (Naive Bayes)、高斯混合模型 (GMM)、隐马尔可夫模型 (HMM)、贝叶斯网络、生成对抗网络 (GAN),,。

---

# 2 有监督学习 vs. 无监督学习 (Supervised vs. Unsupervised)

主要区别在于训练数据**是否有标签（Label）**。

- **有监督学习 (Supervised Learning)**
    
    - **定义**：训练数据包含输入特征 $X$ 和对应的标签 $Y$。目标是学习一个模型，能够对新的未知输入预测其输出,,。
    - **两大任务**：
        - **分类 (Classification)**：输出变量 $Y$ 是离散的（如垃圾邮件识别、图像分类）,。
        - **回归 (Regression)**：输出变量 $Y$ 是连续的数值（如房价预测、气温预测）,。
    - **典型算法**：线性回归、逻辑回归、SVM、决策树、神经网络,。
- **无监督学习 (Unsupervised Learning)**
    
    - **定义**：训练数据只有输入特征 $X$，**没有标签 $Y$**。目标是发现数据内部的结构、模式或规律,,。
    - **主要任务**：
        - **聚类 (Clustering)**：将相似的样本划分为一组（如K-Means、DBSCAN、GMM）,。
        - **降维 (Dimensionality Reduction)**：将高维数据映射到低维空间，保留主要特征（如PCA、LDA）。
        - **概率密度估计**：估计数据的分布情况。

---

# 3 集成学习：Bagging vs. Boosting

集成学习通过组合多个弱学习器来构建强学习器，核心差异在于**训练方式（并行/串行**和**优化目标（方差/偏差）**。

|维度|**Bagging (如随机森林)**|**Boosting (如 AdaBoost, GBDT)**|
|:--|:--|:--|
|**训练方式**|**并行 (Parallel)**：各个基学习器之间没有依赖关系，可以同时训练,。|**串行 (Serial)**：基学习器存在强依赖关系，后一个模型基于前一个模型的错误进行训练,。|
|**样本采样**|**自助采样 (Bootstrap)**：有放回地随机采样，每个模型的训练集不同,。|**全样本/加权**：通常使用全部样本，但会调整样本的**权重**（分错的样本权重增加）,。|
|**核心作用**|主要降低 **方差 (Variance)**：通过平均多个模型减少过拟合，提高稳定性。|主要降低 **偏差 (Bias)**：通过逐步聚焦错分样本，提高模型的拟合能力。|
|**权重分配**|所有基学习器**权重相等**（投票或平均）,。|根据错误率分配权重，表现好的模型**权重更高**,。|
|**代表算法**|Random Forest (随机森林),。|AdaBoost, GBDT, XGBoost,。|

---

# 4 正则化 (Regularization) 的作用

正则化是防止模型**过拟合 (Overfitting)**、提高泛化能力的关键技术，体现了结构风险最小化,,。

1. **核心作用**：在损失函数中增加一个**惩罚项 (Penalty Term)**，限制模型的复杂度（限制权重的大小或数量），防止模型对训练数据“死记硬背”,,。
    
2. **L1 正则化 (Lasso)**：
    
    - **公式**：$\lambda \sum |w_i|$ （权重的绝对值之和）,。
    - **作用**：可以使部分权重变为 **0**，产生**稀疏解**。
    - **应用**：用于**特征选择**，去除不重要的特征,。
    - **几何解释**：约束区域是棱形，等高线易与坐标轴相交,。
3. **L2 正则化 (Ridge / 岭回归)**：
    
    - **公式**：$\lambda \sum w_i^2$ （权重的平方和）,。
    - **作用**：使权重普遍变小（**权重衰减**），但不会减为0。
    - **应用**：用于处理多重共线性，使模型函数曲线更**平滑**，防止过拟合,。
    - **几何解释**：约束区域是圆形,。
4. **Dropout (深度学习专用)**：
    
    - 在训练过程中随机“丢弃”一部分神经元（将其输出置为0），相当于训练了多个不同的神经网络并进行集成，有效防止深度神经网络的过拟合,。