# 1 核心模型深度解析 (Deep Dive into Models)

## 1.1 感知机 (Perceptron) —— 线性分类的起点

- **数学定义**：假设输入空间（特征空间）是 $\mathcal{X} \subseteq \mathbb{R}^n$，输出空间是 $\mathcal{Y} = {+1, -1}$。感知机通过学习权值向量 $w$ 和偏置 $b$，构建分离超平面 $w \cdot x + b = 0$。
- **损失函数 (Loss Function)**：感知机不使用分类错误的数量作为损失（因为不可导），而是使用**误分类点到超平面的总距离**。
    - 对于误分类点 $(x_i, y_i)$，有 $-y_i(w \cdot x_i + b) > 0$。
    - 损失函数定义为所有误分类点 $M$ 的距离之和： $$L(w, b) = -\sum_{x_i \in M} y_i(w \cdot x_i + b)$$
- **优化算法 (SGD)**：
    - 计算梯度：$\nabla_w L = -\sum_{x_i \in M} y_i x_i$，$\nabla_b L = -\sum_{x_i \in M} y_i$。
    - **更新公式**（随机梯度下降，每次只取一个误分类点更新）： $$w \leftarrow w + \eta y_i x_i$$ $$b \leftarrow b + \eta y_i$$
    - **直观理解**：如果一个正样本（$y_i=+1$）被错误预测为负，$w \cdot x + b$ 太小了，需要加上 $\eta x_i$ 让 $w$ 变大，从而让结果向正方向移动。

## 1.2 支持向量机 (SVM) —— 几何间隔最大化

- **核心目标**：寻找最大间隔（Max Margin）。几何间隔为 $\gamma = \frac{1}{||w||}$。最大化间隔等价于最小化 $\frac{1}{2}||w||^2$。
- **对偶问题 (Dual Problem)**：
    - 原始问题包含不等式约束，引入拉格朗日乘子 $\alpha_i \ge 0$，构造拉格朗日函数 $L(w, b, \alpha)$。
    - **对偶形式**： $$ \max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) $$
    - **约束条件**：$\sum_{i=1}^N \alpha_i y_i = 0$，$\alpha_i \ge 0$。
    - **解的稀疏性**：只有支持向量对应的 $\alpha_i > 0$，其他样本的 $\alpha_i = 0$。这也是SVM高效的原因之一。
- **核技巧 (Kernel Trick)**：
    - 当数据线性不可分时，将数据映射到高维空间 $\phi(x)$。
    - 在对偶形式中，只涉及向量内积 $(x_i \cdot x_j)$。核函数可以直接计算高维空间的内积，避免了显式计算 $\phi(x)$： $$K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$$
    - **常用核函数**：
        - 多项式核：$K(x, z) = (x \cdot z + 1)^p$
        - 高斯核 (RBF)：$K(x, z) = \exp(-\frac{||x-z||^2}{2\sigma^2})$。

## 1.3 朴素贝叶斯 (Naive Bayes) —— 概率模型的简化

- **核心公式**：基于贝叶斯定理。 $$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$$
- **“朴素”假设**：假设特征之间**相互独立**。即 $P(X|Y) = P(x_1, x_2, ..., x_n|Y) = \prod_{i=1}^n P(x_i|Y)$。
- **拉普拉斯平滑 (Laplacian Smoothing)**：
    - 如果某个特征值在训练集中没出现过，概率为0，会导致连乘结果为0。
    - **修正公式**：$P_\lambda(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum I(...) + \lambda}{\sum I(...) + S_j \lambda}$。通常取 $\lambda=1$。

## 1.4 决策树 (Decision Tree) —— 信息论的应用

- **信息熵 (Entropy)**：度量样本集合纯度。$Ent(D) = -\sum_{k=1}^{|y|} p_k \log_2 p_k$。值越小，纯度越高。
- **三种核心算法对比**：
    1. **ID3**：使用**信息增益 (Information Gain)**。 $$Gain(D, a) = Ent(D) - \sum \frac{|D^v|}{|D|} Ent(D^v)$$ _缺点_：偏向于选择取值较多的属性（如“编号”）。
    2. **C4.5**：使用**信息增益率 (Gain Ratio)**。 $$Gain_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}$$ 其中 $IV(a)$ 是属性 $a$ 的固有值，属性取值越多，$IV(a)$ 越大，从而惩罚了取值多的属性。
    3. **CART**：使用**基尼指数 (Gini Index)**。 $$Gini(D) = 1 - \sum p_k^2$$ CART构建的是二叉树，选择使划分后基尼指数最小的属性。

---

# 2 二、 神经网络与深度学习 (Neural Networks) —— 【计算题核心】

这部分是您特别强调的“重中之重”和“必考计算”。

## 2.1 前向传播 (Forward Propagation)

假设一个简单的三层网络：输入 $x$，隐层 $h$，输出 $y$，激活函数为 $\sigma$（如Sigmoid）。

- **隐层计算**： $$z_1 = w_1 x + b_1$$ $$a_1 = \sigma(z_1)$$
- **输出层计算**： $$z_2 = w_2 a_1 + b_2$$ $$a_2 = \sigma(z_2)$$
- **损失函数**（以均方误差为例）：$L = \frac{1}{2}(y_{true} - a_2)^2$。

## 2.2 反向传播 (Backpropagation) —— 链式法则求梯度

我们要更新权重 $w_2$ 和 $w_1$，必须求出 $\frac{\partial L}{\partial w_2}$ 和 $\frac{\partial L}{\partial w_1}$。

- **Step 1: 求输出层权重的梯度** $$ \frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_2} $$
    
    - $\frac{\partial L}{\partial a_2} = -(y_{true} - a_2)$
    - $\frac{\partial a_2}{\partial z_2} = \sigma'(z_2) = a_2(1-a_2)$ （Sigmoid导数性质）
    - $\frac{\partial z_2}{\partial w_2} = a_1$
    - **结果**：$\delta_2 = (a_2 - y_{true})a_2(1-a_2)$，则梯度 $\nabla w_2 = \delta_2 \cdot a_1$。
- **Step 2: 求隐层权重的梯度** (误差回传) $$ \frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1} $$
    
    - $\frac{\partial L}{\partial z_2} = \delta_2$ (上面已求)
    - $\frac{\partial z_2}{\partial a_1} = w_2$
    - $\frac{\partial a_1}{\partial z_1} = \sigma'(z_1) = a_1(1-a_1)$
    - $\frac{\partial z_1}{\partial w_1} = x$
    - **结果**：$\delta_1 = (\delta_2 \cdot w_2) \cdot a_1(1-a_1)$，则梯度 $\nabla w_1 = \delta_1 \cdot x$。
- **Step 3: 参数更新** $$ w_{new} = w_{old} - \eta \cdot \nabla w $$
    

## 2.3 深度学习组件

- **CNN (卷积神经网络)**：
    - **卷积 (Convolution)**：提取局部特征，共享权重。公式：$h(i,j) = \sum \sum f(m,n)g(i-m, j-n)$。
    - **池化 (Pooling)**：降维，保留主要特征（Max Pooling取最大值，Mean Pooling取平均）。
- **RNN/LSTM**：
    - **RNN**：$h_t = f(U x_t + W h_{t-1} + b)$，存在梯度消失问题。
    - **LSTM**：引入**门控机制**（遗忘门、输入门、输出门）来控制信息流，解决长距离依赖问题。

---

# 3 三、 优化与正则化 (Optimization & Regularization)

## 3.1 损失函数 (Loss Function)

- **回归**：MSE (均方误差) $L = \frac{1}{N}\sum (y - \hat{y})^2$，对异常值敏感。
- **分类**：
    - **交叉熵 (Cross-Entropy)**：$L = -\sum y \log \hat{y}$。配合Softmax使用，梯度计算简单。
    - **Hinge Loss**：$L = \max(0, 1 - y \cdot f(x))$，SVM专用。

## 3.2 正则化 (Regularization) —— 防止过拟合

- **L1 (Lasso)**：$J(w) + \lambda ||w||_1$。
    - **几何解释**：等值线是菱形，容易与坐标轴相交，产生**稀疏解**（某些权重变为0），用于特征选择。
- **L2 (Ridge)**：$J(w) + \lambda ||w||_2^2$。
    - **几何解释**：等值线是圆形，使权重普遍变小但不为0（权重衰减），防止过拟合。

## 3.3 集成学习 (Ensemble)

- **Bagging**：并行。**随机森林**是代表。通过Bootstrap采样（有放回）训练多个基学习器，投票或平均。主要降低**方差**（抗噪能力强）。
- **Boosting**：串行。**AdaBoost**是代表。
    - **权重更新**：上一轮分错的样本，权重增加；分对的样本，权重减小。
    - **主要作用**：降低**偏差**（拟合能力强）。

---

# 4 四、 无监督学习详细推导

## 4.1 K-Means 聚类

- **目标函数**：最小化簇内平方误差。 $$ E = \sum_{i=1}^k \sum_{x \in C_i} ||x - \mu_i||^2 $$
- **EM思想**：
    - E步：固定中心 $\mu_i$，将每个样本分配到最近的簇。
    - M步：固定簇分配，更新 $\mu_i$ 为簇内样本均值。

## 4.2 PCA (主成分分析)

- **目标**：找到投影方向 $w$，使得投影后的数据方差最大。
- **推导**：
    1. 数据中心化。
    2. 计算协方差矩阵 $C = \frac{1}{m} X X^T$。
    3. 对 $C$ 进行特征值分解 $C = U \Sigma U^T$。
    4. 取前 $k$ 个最大特征值对应的特征向量组成投影矩阵 $P$。

## 4.3 GMM (高斯混合模型)

- **模型**：假设数据是由 $K$ 个高斯分布混合生成的。$P(x) = \sum \alpha_k \mathcal{N}(x|\mu_k, \Sigma_k)$。
- **求解 (EM算法)**：
    - **E步**：计算每个样本属于第 $k$ 个高斯分布的后验概率（责任度）$\gamma_{ik}$。
    - **M步**：利用 $\gamma_{ik}$ 加权更新每个高斯分布的参数 $\mu_k, \Sigma_k, \alpha_k$。

---

### 4.3.1 五、 关键对比总结 (Key Comparisons)

|对比项|**生成式模型 (Generative)**|**判别式模型 (Discriminative)**|
|:--|:--|:--|
|**学习目标**|联合概率 $P(X,Y)$|条件概率 $P(Y|
|**典型算法**|朴素贝叶斯, GMM, HMM|SVM, 逻辑回归, 神经网络|
|**优点**|数据量少时收敛快，可生成数据|准确率通常更高，直接优化分类边界|

| 对比项      | **L1 正则化**                  | **L2 正则化**             |
| :------- | :-------------------------- | :--------------------- |
| **惩罚项**  | $\sum$                      | $w_i$                  |
| **解的特性** | **稀疏解** (Feature Selection) | **平滑解** (Weight Decay) |
| **几何形状** | 菱形 (Diamond)                | 圆形 (Circle)            |

这份详细的讲解覆盖了从基础公式推导到模型内部机制的各个方面，特别是**BP算法的链式法则**和**SVM的对偶问题**，是考试和理解的难点，建议重点复习。