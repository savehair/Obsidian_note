# 1 一、 核心基础概念 (Core Concepts)

这部分建立了机器学习的宏观认知。

1. **机器学习的定义与定位**
    
    - **关系**：机器学习是人工智能（AI）的一个核心分支，是实现AI的手段。数据挖掘利用机器学习算法从海量数据中发现规律。
    - **三要素**：统计学习方法由 **模型（Model）**、**策略（Strategy）** 和 **算法（Algorithm）** 构成。
        - **模型**：要学习的假设空间（如线性函数、神经网络）。
        - **策略**：评估模型好坏的标准（如损失函数）。
        - **算法**：求解最优参数的计算方法（如梯度下降）。
2. **核心定理：没有免费的午餐 (NFL 定理)**
    
    - **内容**：如果对所有可能的问题同等看待，那么所有算法的性能是一样的。脱离具体问题谈论算法好坏没有意义。**不存在万能的算法**，必须结合具体场景选择模型。

---

# 2 二、 模型架构 (Model Architecture) ——【核心模块】
## 2.1 基础模型 (Basic Models)

**A. 感知机 (Perceptron)**

- **原理**：神经网络的基石，用于二分类的线性模型。
- **模型公式**：$f(x) = \text{sign}(w \cdot x + b)$。其中 $\text{sign}$ 是符号函数，输出 +1 或 -1。
- **局限性**：只能解决线性可分问题，无法解决 **异或 (XOR)** 问题。
- **优化**：采用随机梯度下降，误分类点驱动参数更新： $$w \leftarrow w + \eta y_i x_i$$ $$b \leftarrow b + \eta y_i$$ 其中 $\eta$ 是学习率。

**B. 支持向量机 (SVM)**

- **核心思想**：在特征空间中寻找一个超平面，使得两类样本的**间隔（Margin）最大化**。
- **数学推导 (硬间隔)**：
    - 原始目标：最大化几何间隔 $\frac{1}{||w||}$，等价于最小化 $\frac{1}{2}||w||^2$。
    - 约束条件：$y_i(w^T x_i + b) \ge 1$。
    - **对偶问题**：引入拉格朗日乘子 $\alpha_i$，将原问题转化为对偶问题求解： $$ \max_\alpha \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) $$ 约束：$\sum \alpha_i y_i = 0, \alpha_i \ge 0$。
- **关键技术**：
    - **核技巧 (Kernel Trick)**：解决非线性不可分问题。将低维数据映射到高维空间（如径向基核 RBF），使其线性可分。$K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$。
    - **软间隔**：引入松弛变量 $\xi$，允许少量样本分类错误，防止过拟合。

**C. 神经网络与深度学习 (Neural Networks & Deep Learning)**

- **多层感知机 (MLP)**：通过引入隐藏层和非线性激活函数（如 Sigmoid, ReLU），解决了感知机无法处理非线性（如 XOR）的问题。
- **核心算法：BP算法 (反向传播)**
    - **推导**：基于链式法则计算损失函数 $L$ 对权重 $w$ 的梯度。
    - 假设输出层误差为 $\delta$，则权重的梯度为：$\frac{\partial L}{\partial w} = \delta \cdot \text{input}$。误差从输出层向输入层反向传播。
- **进阶模型**：
    - **CNN (卷积神经网络)**：核心是**卷积层**（提取局部特征，权重共享）和**池化层**（降维）。适用于图像处理。
    - **RNN/LSTM**：用于处理序列数据（如自然语言）。LSTM（长短期记忆网络）通过引入**遗忘门、输入门、输出门**，解决了传统RNN的长距离依赖和梯度消失问题。
    - **Transformer**：基于**自注意力机制 (Self-Attention)**，并行计算能力强，是BERT等模型的基础。
    - **GAN (生成对抗网络)**：生成器 (G) 和判别器 (D) 相互博弈。G 试图生成逼真样本，D 试图区分真假，最终达到纳什均衡。

## 2.2 决策树与集成学习 (Ensemble Learning)

- **决策树**：基于树结构进行决策。核心是选择最优划分属性。
    - **划分标准**：
        - ID3：**信息增益** (Information Gain)，基于信息熵 $Ent(D) = -\sum p_k \log_2 p_k$。
        - C4.5：**信息增益率** (Gain Ratio)，解决ID3偏向多值属性的问题。
        - CART：**基尼指数** (Gini Index)。
- **集成学习**：结合多个弱学习器构建强学习器。
    - **Boosting (串行)**：关注被错误分类的样本，降低**偏差 (Bias)**。
        - **AdaBoost**：提高错误样本的权重。权重更新公式：$\alpha_t = \frac{1}{2} \ln(\frac{1-\epsilon_t}{\epsilon_t})$。
        - **GBDT/XGBoost**：基于残差学习，每一棵树都在拟合上一棵树的残差。
    - **Bagging (并行)**：自助采样 (Bootstrap)，降低**方差 (Variance)**。
        - **随机森林 (Random Forest)**：Bagging + 随机属性选择。

## 2.3 模型类型区分

- **生成式模型 (Generative)**：学习联合概率分布 $P(X,Y)$，如朴素贝叶斯、GMM。能生成数据。
- **判别式模型 (Discriminative)**：直接学习条件概率 $P(Y|X)$ 或决策边界，如SVM、逻辑回归、神经网络。

## 2.4 无监督学习 (Unsupervised Learning)

- **聚类 (Clustering)**：
    - **K-Means**：迭代计算质心。目标是最小化平方误差 $E = \sum ||x - \mu_i||^2$。
    - **GMM (高斯混合模型)**：假设数据由多个高斯分布混合生成，使用 **EM算法** 求解。
- **降维 (Dimensionality Reduction)**：
    - **PCA (主成分分析)**：将数据投影到方差最大的方向。
    - **推导**：对协方差矩阵进行特征值分解，取最大的 $k$ 个特征值对应的特征向量作为新基。

## 2.5 概率模型

- **朴素贝叶斯 (Naive Bayes)**：
    - **假设**：特征之间相互**独立**。
    - **公式**：$P(y|x) = \frac{P(y) \prod P(x_i|y)}{P(x)}$。
- **LDA (线性判别分析)**：
    - **思想**：投影后**类内方差最小 ($S_w$)，类间方差最大 ($S_b$)**。
    - **目标函数**：$J(w) = \frac{w^T S_b w}{w^T S_w w}$。

---

# 3 策略 (Strategy: Loss & Regularization)

## 3.1 损失函数 (Loss Function)

- **均方误差 (MSE)**：$L = \frac{1}{n}\sum (y - \hat{y})^2$，用于回归任务。
- **交叉熵 (Cross-Entropy)**：$L = -\sum y \log \hat{y}$，用于分类任务（配合Softmax或Sigmoid）。
- **Huber Loss**：结合了MSE和绝对值误差，对异常值（Outliers）更鲁棒。

## 3.2 正则化 (Regularization)

用于防止过拟合。

- **L1 正则化 (Lasso)**：加入 $\lambda ||w||_1$。特点是能产生**稀疏解**，用于特征选择。
- **L2 正则化 (Ridge)**：加入 $\lambda ||w||_2^2$。特点是**权重衰减**，防止参数过大。

## 3.3 参数估计

- **极大似然估计 (MLE)**：最大化观测数据出现的概率 $P(D|\theta)$。
- **最大后验估计 (MAP)**：引入先验概率，最大化 $P(\theta|D) \propto P(D|\theta)P(\theta)$。

---

# 4 算法 (Algorithms: Optimization)
## 4.1 梯度下降法 (Gradient Descent)

核心公式：$\theta_{t+1} = \theta_t - \eta \nabla J(\theta)$。

- **批量梯度下降 (BGD)**：用所有样本计算梯度，准但慢。
- **随机梯度下降 (SGD)**：用一个样本计算梯度，快但震荡。
- **小批量梯度下降 (Mini-batch)**：折中方案，深度学习常用。

## 4.2 其他优化方法

- **牛顿法**：利用二阶导数（Hessian矩阵），收敛快但计算量大。
- **坐标轴下降法**：每次只更新一个坐标轴的参数，适用于L1正则化求解。

## 4.3 BP 反向传播 (Backpropagation)

- **【必考推导】**：利用链式法则。
    - **前向传播**：输入 $\to$ 隐层 $\to$ 输出层。
    - **反向传播**：计算误差对权重的偏导数。例如对于 Sigmoid 激活函数，$\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot (a^{(l)}(1-a^{(l)}))$。

---

# 5 五、 模型评估与分析 (Evaluation)

- **指标**：
    - **准确率 (Accuracy)**：分类正确的比例。
    - **查准率 (Precision)**：$TP / (TP + FP)$。
    - **查全率 (Recall)**：$TP / (TP + FN)$。
    - **F1值**：Precision和Recall的调和平均。
- **偏差-方差分解 (Bias-Variance Tradeoff)**：
    - **偏差**：度量模型的拟合能力（欠拟合时偏差大）。
    - **方差**：度量模型的稳定性（过拟合时方差大）。
- **交叉验证 (Cross Validation)**：k折交叉验证，用于评估模型泛化能力。

---

# 6 六、 重点考核标准 (Key Exam Points)

根据思维导图和PPT的重点标记：

1. **计算题**：必须掌握 **BP神经网络的反向传播计算**（链式法则求梯度） 和 **梯度下降** 的参数更新过程。
2. **概念区分**：生成式与判别式模型的区别；L1与L2正则化的几何解释与区别。
3. **核心原理**：SVM的核函数作用、集成学习中Bagging与Boosting的区别（方差 vs 偏差）。

**简单例子：**

- **决策树**：判断是否去打球。根节点可以是“天气”，如果“晴朗”则看“湿度”，如果“下雨”则看“风速”。
- **SVM**：桌子上有红球和蓝球，用一根棍子（超平面）把它们分开，并让棍子离两边的球都尽可能远（最大间隔）。如果球混在一起，就拍一下桌子让球飞起来（核函数映射到高维），在空中用一张纸分开它们。