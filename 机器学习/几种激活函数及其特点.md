# 1 Sigmoid 函数

这是早期最常用的激活函数，也被称为 Logistic 函数。

- **公式**：$\sigma(z) = \frac{1}{1+e^{-z}}$
- **特点**：
    - **输出范围**：将输入映射到 $(0, 1)$ 区间，输出值可被视为概率。
    - **优点**：函数光滑、连续可导，适合用于二分类问题的输出层。
    - **缺点**：
        - **梯度消失（Gradient Vanishing）**：当输入值非常大或非常小时，导数趋近于 0，导致反向传播时梯度无法有效传递，深层网络难以训练。
        - **非零均值（Non-Zero Centered）**：输出恒为正，导致收敛速度变慢。
        - **计算昂贵**：涉及指数运算，计算量较大。

# 2 Tanh (双曲正切) 函数

Tanh 函数是 Sigmoid 函数的变体，形状相似但经过了平移和拉伸。

- **公式**：$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$
- **特点**：
    - **输出范围**：映射到 $(-1, 1)$ 区间。
    - **优点**：输出是以 **0 为中心（Zero-Centered）** 的，这使得其收敛速度通常比 Sigmoid 快。
    - **缺点**：仍然存在**梯度消失**问题，当输入很大或很小时，梯度趋近于 0。

# 3 ReLU (修正线性单元) 函数

目前深度学习（尤其是 CNN）中**最常用**的激活函数。

- **公式**：$f(x) = \max(0, x)$
- **特点**：
    - **输出范围**：$[0, +\infty)$，是非饱和函数。
    - **优点**：
        - **解决梯度消失**：在 $x > 0$ 时导数恒为 1，能够有效缓解梯度消失问题，加速收敛。
        - **计算高效**：只需判断阈值，无指数运算，训练速度极快。
        - **稀疏性**：使一部分神经元输出为 0，增加了网络的稀疏性。
    - **缺点**：
        - **Dead ReLU（死亡神经元）**：当 $x < 0$ 时，梯度为 0，该神经元在后续训练中可能永远不会被激活，导致参数无法更新。
        - **输出非零均值**：输出恒大于等于 0。

# 4 ReLU 的变体 (Leaky ReLU, PReLU, ELU)

为了解决 Dead ReLU 问题，衍生出了多种变体：

- **Leaky ReLU (带泄露的 ReLU)**：
    - **特点**：在 $x < 0$ 时给予一个很小的斜率（如 $\alpha=0.01$），而不是直接置 0。这保证了负区间也有梯度，神经元不会“死亡”。
- **PReLU (参数化 ReLU)**：
    - **特点**：负区间的斜率 $\alpha$ 不是固定的，而是作为一个可学习的参数在训练中自动调整。
- **ELU (指数线性单元)**：
    - **公式**：$x > 0$ 时为 $x$；$x \le 0$ 时为 $\alpha(e^x - 1)$。
    - **特点**：融合了 Sigmoid 和 ReLU 的优点，左侧具有软饱和性，对噪声更鲁棒，且输出均值接近于 0。缺点是计算包含指数，开销较大。

# 5 Softmax 函数

通常用于多分类神经网络的**输出层**。

- **公式**：$S_i = \frac{e^{z_i}}{\sum e^{z_j}}$
- **特点**：
    - **归一化**：将多个神经元的输出映射到 $(0, 1)$ 区间，且所有输出之和为 1。
    - **物理意义**：输出值可直接理解为属于某一类别的**概率**。
    - **放大差异**：通过指数运算，会让大的值更大，小的值更小，使分类结果更明显。

# 6 Maxout 函数

- **特点**：它是 ReLU 和 Leaky ReLU 的一般化形式，通过计算 $k$ 个线性函数的最大值来得到输出。
- **优点**：拟合能力强，没有“死亡神经元”问题。
- **缺点**：参数量是普通神经元的 $k$ 倍，计算量大，导致模型体积膨胀。

# 7 其他前沿函数

- **Softplus**：$f(x) = \ln(1+e^x)$，可以看作是 ReLU 的平滑版本，处处可导，但计算量大。
- **Mish**：$f(x) = x \cdot \text{tanh}(\ln(1+e^x))$，这是一种非单调的平滑激活函数，允许微小的负值输出，在某些深度网络中表现优于 ReLU。

# 8 总结与选择建议

- **二分类问题**：输出层首选 **Sigmoid**。
- **多分类问题**：输出层首选 **Softmax**。
- **隐层（中间层）**：
    - 首选 **ReLU**，因为它计算快且能==防止梯度消失==。
    - 如果出现“死亡神经元”导致模型不收敛，可尝试 **Leaky ReLU** 或 **Maxout**。
    - 通常应**避免**在隐层使用 Sigmoid，因为它极易导致梯度消失。
    - **Tanh** 的效果通常优于 Sigmoid，但不如 ReLU。